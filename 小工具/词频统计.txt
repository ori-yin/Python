词频统计
方法1
import jieba    # 调用jieba库

txt = open('740879160.txt','r',encoding='UTF-8').read()    # 打开文件open，r只读，encoding转化编码

words = jieba.lcut(txt)    # jieba库的方法lcut分词

count = {}    # 赋值变量等于字典
for word in words:    # 遍历分词后的结果
    if len(word) == 1:    # word是遍历后的结果，当len(word)等于1的时候跳过继续循环
        continue    # continue是跳过，break是终止循环
    else:
        count[word] = count.get(word,0) +1    # 匹配到则+1，否则+0

item = list(count.items())    # 由于字典没有排序sort，所以转化为列表list
item.sort(key=lambda x:x[1],reverse=True)    # lambda表达式，按key排序，reverse=True为升序

for i in range(500):    # 遍历前N个
    word,count = item[i]    # word=key,count=value，字典键值对赋值
    # print('{0:<10}{1:>5}'.format(word,count))    # 输出结束，网上抄的，不理解做法
    print(f'{word}:{count}')    # 输出结果，可以替换：以便后期分列

方法2（更快）
import jieba    # 导入库

txt = open(f'G:\杂七杂八\951026.txt','r',encoding='UTF=8').read()    # open打开文件，r只读，编码转化UTF=8

txt2 = jieba.lcut(txt)    # lcut进行分词

miao = {}    # 赋值一个空字典
for i in txt2:    # 遍历分词结果
    if len(i) == 1:    # 筛选遍历结果，等于1的continue跳过
        continue
    else:
        miao[i] = miao.get(i,0) +1    # 满足条件的get到一个加1，否则加0

x = sorted(miao.items(),key=lambda x:x[1] ,reverse=True)    # 字典排序，items用于输出字典键值对

# for y,z in x:    # 新建x,y变量，并遍历排序后的x，x=keys,y=values
#   print(f'{y}={z}')   # 等号可以替换，方便后期分组

x2 =pd.DataFrame(data=x,columns=['关键词','次数'],index=None)
x2.to_csv('G:\杂七杂八\桑基图学习\词云图.csv')    # 保持到本地文件
print(x2)
print('导出成功')